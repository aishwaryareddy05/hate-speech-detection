# -*- coding: utf-8 -*-
"""HateSpeechDetectionModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Z7n60-rr_clrbqvWj_ZzcZyMWTKVDRk
"""

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
from imblearn.over_sampling import SMOTE
from sklearn.decomposition import TruncatedSVD

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Load dataset
data = pd.read_csv("twitter.csv")

# Data Cleaning and Preprocessing
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    words = [word for word in text.split() if word not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    text = " ".join([lemmatizer.lemmatize(word) for word in words])
    return text

# Apply cleaning
data["clean_tweet"] = data["tweet"].apply(clean_text)

# Define features and labels
X = data["clean_tweet"]
y = data["class"].map({0: "Hate Speech", 1: "Offensive Language", 2: "No Hate and Offensive"})

# Convert text into TF-IDF features
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=20000)
X_tfidf = tfidf_vectorizer.fit_transform(X)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_tfidf_resampled, y_resampled = smote.fit_resample(X_tfidf, y)

# Reduce dimensions for faster training
svd = TruncatedSVD(n_components=500, random_state=42)
X_tfidf_reduced = svd.fit_transform(X_tfidf_resampled)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf_reduced, y_resampled, test_size=0.33, random_state=42)

# Train Logistic Regression model
log_reg_model = LogisticRegression(max_iter=1000)
log_reg_model.fit(X_train, y_train)

# Save the model and vectorizer
joblib.dump(log_reg_model, "log_reg_model.pkl")
joblib.dump(tfidf_vectorizer, "tfidf_vectorizer.pkl")
joblib.dump(svd, "svd_transform.pkl")

# Model Testing
predictions = log_reg_model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print("Model Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, predictions))

# Sample Predictions with Adjusted Threshold
def test_sample_texts():
    test_texts = [
        "I hate this person so much!",
        "You are so dumb and useless!",
        "Have a great day everyone!",
        "That was an awesome match!",
        "I want to kill him",
        "I don't like her"
    ]
    transformed_texts = svd.transform(tfidf_vectorizer.transform(test_texts))
    proba = log_reg_model.predict_proba(transformed_texts)
    classes = log_reg_model.classes_

    threshold = 0.6  # Adjusted threshold for Hate Speech
    results = []
    for prob in proba:
        if prob[classes.tolist().index("Hate Speech")] >= threshold:
            results.append("Hate Speech")
        else:
            results.append(classes[np.argmax(prob)])

    for text, pred in zip(test_texts, results):
        print(f"Text: {text}\n\u279e Prediction: {pred}\n")

test_sample_texts()

def test_sample_texts():
    test_texts = [
        "I hate this person so much!",
        "You are so dumb !",
        "Have a great day everyone!",
        "That was an awesome match!",
        "I want to kill him",
        "I don't like her"
    ]
    transformed_texts = svd.transform(tfidf_vectorizer.transform(test_texts))
    results = log_reg_model.predict(transformed_texts)
    for text, pred in zip(test_texts, results):
        print(f"Text: {text}\nâžž Prediction: {pred}\n")

test_sample_texts()